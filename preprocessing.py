# -*- coding: utf-8 -*-
"""tes npl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VbPVr3_eaRLLHx47pwr-nQowsa_xhDF3

#Import Libraries
"""

import pandas as pd

from google.colab import drive
# from sklearn.preprocessing import LabelEncoder
# from imblearn.under_sampling import RandomUnderSampler
# from collections import Counter
# from sklearn.model_selection import train_test_split
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense
# from tensorflow.keras.callbacks import EarlyStopping

"""#Cek Dataset(Optional)

### If u have 2 or more dataset and want to compare the label and the data of both
"""

# drive.mount('/content/drive')

# # Path file di Google Drive
# file_path_1 = '/content/drive/MyDrive/capstone/diabetes_prediction_dataset1.csv'
# file_path_2 = '/content/drive/MyDrive/capstone/preprocessed_diabetes_data.csv'

# # Membaca file CSV dari Google Drive
# df1 = pd.read_csv(file_path_1)
# df2 = pd.read_csv(file_path_2)


# # Memeriksa apakah kedua dataset memiliki dimensi yang sama
# if df1.shape == df2.shape:
#     print("Dataset memiliki jumlah baris dan kolom yang sama.")

#     # Membandingkan apakah kedua dataset sama persis
#     if df1.equals(df2):
#         print("Isi kedua dataset sama persis.")
#     else:
#         print("Isi kedua dataset berbeda.")
#         # Menampilkan perbedaan secara lebih mendetail
#         comparison = df1.compare(df2)
#         print("Perbedaan antara kedua dataset:")
#         print(comparison)
# else:
#     print("Dataset memiliki jumlah baris atau kolom yang berbeda.")

"""#Data Gathering"""

drive.mount('/content/drive')

# Path file di Google Drive
file_path = '/content/drive/MyDrive/Dataset/dataset_berita_nlp_100.csv'

# Membaca file CSV dari Google Drive
df = pd.read_csv(file_path)

"""#Preprocessing Data"""

df.head()

df.info()

df.isna().sum()

print(df.isnull().sum())

pip install Sastrawi

import nltk
nltk.download('punkt_tab')
nltk.download('stopwords')
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# Membuat stemmer menggunakan Sastrawi
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Mengambil daftar stopwords bahasa Indonesia
stop_words = set(stopwords.words("indonesian"))

def preprocess_text(text):
    # Mengubah teks menjadi huruf kecil
    text = text.lower()

    # Menghapus angka dan tanda baca
    text = re.sub(r'[^\w\s]', '', text)  # Menghapus tanda baca
    text = re.sub(r'\d+', '', text)      # Menghapus angka

    # Tokenisasi kata
    tokens = word_tokenize(text)

    # Menghapus stopwords dan melakukan stemming
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]

    # Menggabungkan kembali kata-kata yang sudah diproses menjadi sebuah teks
    return " ".join(tokens)

df.head()

df['clean_text'] = df['isi_berita'].apply(preprocess_text)
from sklearn.feature_extraction.text import TfidfVectorizer

# Membuat model TF-IDF
tfidf = TfidfVectorizer()
X_tfidf = tfidf.fit_transform(df["clean_text"])  # df["clean_text"] adalah kolom teks yang sudah diproses

from sklearn.preprocessing import LabelEncoder

# Encoding kategori menjadi numerik
encoder = LabelEncoder()
df['kategori_encoded'] = encoder.fit_transform(df['kategori'])

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

# Membagi data menjadi training dan testing
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df["kategori_encoded"], test_size=0.2)

# Membuat model Na√Øve Bayes
model = MultinomialNB()

# Melatih model
model.fit(X_train, y_train)

# Prediksi pada data test
y_pred = model.predict(X_test)

# Evaluasi model
print("Akurasi Model: ", accuracy_score(y_test, y_pred))

import joblib

# Simpan model yang telah dilatih
joblib.dump(model, "model.pkl")

# Simpan TF-IDF vectorizer
joblib.dump(tfidf, "tfidf.pkl")

